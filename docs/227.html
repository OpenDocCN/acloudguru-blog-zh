<html>
<head>
<title>Azure Event Hubs and Stream Analytics: Crossing the Stream | A Cloud Guru</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Azure事件中心和流分析:穿越流</h1>
<blockquote>原文：<a href="https://acloudguru.com/blog/engineering/crossing-the-streams-with-azure-event-hubs-and-stream-analytics#0001-01-01">https://acloudguru.com/blog/engineering/crossing-the-streams-with-azure-event-hubs-and-stream-analytics#0001-01-01</a></blockquote><div><div class="elementor-widget-container"><p>由Abhishek Gupta <br/> <a href="https://medium.com/@abhishek1987" target="_blank" rel="noreferrer noopener">博客</a> | <a href="https://in.linkedin.com/in/abhirockzz" target="_blank" rel="noreferrer noopener"> LinkedIn </a> | <a href="https://twitter.com/abhi_tweeter" target="_blank" rel="noreferrer noopener">推特</a></p><p>这篇博客提供了一个如何使用<a href="https://docs.microsoft.com/azure/stream-analytics/stream-analytics-introduction?WT.mc_id=acloudguru-blog-abhishgu" target="_blank" rel="noreferrer noopener"> Azure Stream Analytics </a>处理来自<a href="https://docs.microsoft.com/azure/event-hubs/event-hubs-about?WT.mc_id=acloudguru-blog-abhishgu" target="_blank" rel="noreferrer noopener"> Azure Event Hubs </a>的流数据的实际例子。您应该能够使用Azure门户(或Azure CLI)完成本教程，而无需编写任何代码。在这篇博文的最后，还有其他资源可以用来探索使用Azure Stream Analytics进行流处理。</p><p><strong>包括哪些内容？</strong></p><ul><li>用例、解决方案及其组成部分的快速概述</li><li>如何设置所需的Azure服务:事件中心、流分析和Blob存储</li><li>如何使用示例数据配置和测试Azure流分析作业</li><li>如何运行Azure流分析作业并使用实时数据进行测试</li></ul><p class="has-text-align-center"><em>想了解更多关于Azure认证的信息吗？</em> <br/> <em>查看我们的<a href="https://acloudguru.com/azure-cloud-training" target="_blank" rel="noreferrer noopener"> Azure认证和学习路径。</a> </em></p><h2 id="h-overview"><strong>概述</strong></h2><p>Azure Stream Analytics是一个实时分析和复杂事件处理引擎，旨在同时分析和处理来自多个来源的大量快速流数据。它支持一个<code>Job</code>的概念，每个由一个<code>input</code>、<code>query</code>和一个<code>output</code>组成。Azure Stream Analytics可以从Azure Event Hub(包括Apache Kafka的Azure Event Hub)、Azure IoT Hub或Azure Blob存储中<code>ingest</code>数据。<code>query</code>基于SQL查询语言，可用于轻松过滤、排序、聚合和连接一段时间内的流数据。</p><p>假设您有一个应用程序，它接受来自客户的已处理订单，并将它们发送到Azure Event Hubs。该要求是处理“原始”订单数据，并用额外的客户信息(如姓名、电子邮件、位置等)丰富它。要做到这一点，您可以构建一个下游服务，该服务将从事件中心消费这些订单并处理它们。在这个例子中，这个服务恰好是一个Azure流分析作业(当然，我们将在后面探讨它！)</p><hr class="wp-block-separator"/><p class="has-text-align-center">作为基础设施的一部分，您可以使用Kafka来处理消息传递。想了解更多关于使用简单HTTP请求生成和使用消息的知识吗？尝试我们的动手实验，获得使用REST代理消费<a href="https://acloudguru.com/hands-on-labs/consuming-kafka-messages-with-confluent-rest-proxy"> Kafka数据所需的请求经验。</a></p><hr class="wp-block-separator"/><p>为了构建这个应用程序，我们需要从外部系统(例如，一个数据库)获取这个客户数据，对于订单信息中的每个客户ID，我们将查询这个客户的详细信息。这将满足低速数据系统或不需要考虑端到端处理延迟的系统。但这将对高速流数据的实时处理提出挑战。</p><p>当然，这不是一个新奇的问题！这篇博文的目的是展示如何使用Azure Stream Analytics实现一个解决方案。以下是各个组件:</p><h3 id="h-input-data-source">输入数据源</h3><p>Azure流分析作业连接到<a rel="noreferrer noopener" href="https://docs.microsoft.com/azure/stream-analytics/stream-analytics-add-inputs?WT.mc_id=acloudguru-blog-abhishgu" target="_blank">一个或多个数据输入</a>。每个输入定义了一个到现有数据源的连接——在本例中是<a rel="noreferrer noopener" href="https://docs.microsoft.com/azure/stream-analytics/stream-analytics-define-inputs?WT.mc_id=acloudguru-blog-abhishgu#stream-data-from-event-hubs" target="_blank">，它的Azure Event Hubs </a>。</p><p>单个订单是一个JSON有效负载，如下所示:</p><pre class="wp-block-preformatted">{
    "id": "42",
    "custid": "4",
    "amount": "100"
}</pre><h3 id="h-reference-data">参考数据</h3><p>客户信息作为<a href="https://docs.microsoft.com/azure/stream-analytics/stream-analytics-use-reference-data?WT.mc_id=acloudguru-blog-abhishgu" target="_blank" rel="noreferrer noopener">参考数据</a>提供。虽然，客户信息可能会改变(例如，如果客户改变了她的电话号码)，但出于本例的目的，我们将把它视为存储在<a href="https://docs.microsoft.com/azure/stream-analytics/stream-analytics-use-reference-data?WT.mc_id=acloudguru-blog-abhishgu#azure-blob-storage" target="_blank" rel="noreferrer noopener"> Azure Blob存储容器</a>中的<a href="https://docs.microsoft.com/azure/stream-analytics/stream-analytics-use-reference-data?WT.mc_id=acloudguru-blog-abhishgu#static-reference-data" target="_blank" rel="noreferrer noopener"> <code>static</code>引用数据</a>。</p><h3 id="h-query">询问</h3><p>这是我们解决方案的核心！它基于匹配的客户ID(在<code>customers</code>数据集中是<code>id</code>,在<code>orders</code>流中是<code>id</code>)将来自Azure Event Hubs的订单数据(连续流)与静态参考客户数据连接起来</p><h3 id="h-output-sink">输出接收器</h3><p>简而言之，<a rel="noreferrer noopener" href="https://docs.microsoft.com/azure/stream-analytics/stream-analytics-define-outputs?WT.mc_id=acloudguru-blog-abhishgu" target="_blank">输出</a>让您存储和保存流分析作业的结果。在这个例子中，为了简单起见，我们<a href="https://docs.microsoft.com/azure/stream-analytics/stream-analytics-define-outputs?WT.mc_id=acloudguru-blog-abhishgu#event-hubs" target="_blank" rel="noreferrer noopener">继续使用Azure Event Hubs(一个不同的主题)</a>作为输出。</p><p>现在您已经有了一个概念性的概述，是时候深入了解了。你只需要一个Azure账户。如果你还没有，就免费拿一个。</p><h2 id="h-initial-setup">初始设置</h2><p>在本节中，您将:</p><ul><li>创建Azure事件中心命名空间和主题</li><li>创建Azure Blob存储帐户和容器</li><li>创建Azure Stream Analytics作业，并为该作业配置事件中枢和Blob存储输入</li></ul><h3 id="h-azure-event-hubs">Azure活动中心</h3><p>您需要创建一个<a href="https://docs.microsoft.com/azure/event-hubs/event-hubs-features?WT.mc_id=acloudguru-blog-abhishgu#namespace" target="_blank" rel="noreferrer noopener">事件中心名称空间</a>和中心(主题)。有很多选项，包括<a href="https://docs.microsoft.com/azure/event-hubs/event-hubs-create?WT.mc_id=acloudguru-blog-abhishgu" target="_blank" rel="noreferrer noopener"> Azure Portal </a>、<a href="https://docs.microsoft.com/azure/event-hubs/event-hubs-quickstart-cli?WT.mc_id=acloudguru-blog-abhishgu" target="_blank" rel="noreferrer noopener"> Azure CLI </a>、<a href="https://docs.microsoft.com/azure/event-hubs/event-hubs-resource-manager-namespace-event-hub?WT.mc_id=acloudguru-blog-abhishgu" target="_blank" rel="noreferrer noopener"> ARM模板</a>或<a href="https://docs.microsoft.com/azure/event-hubs/event-hubs-quickstart-powershell?WT.mc_id=acloudguru-blog-abhishgu" target="_blank" rel="noreferrer noopener"> Azure PowerShell </a></p><hr class="wp-block-separator"/><p class="has-text-align-center">Azure资源管理器(ARM)模板提供了一种使用文本文件定义Azure资源和配置的强大方法。使用我们的动手实验室来了解如何定位和利用微软的公共<a href="https://acloudguru.com/hands-on-labs/deploy-a-github-quickstart-arm-template-using-the-azure-portal"> Azure quickstart模板。</a>T3】</p><hr class="wp-block-separator"/><p>请注意，您需要创建<strong>两个</strong>主题:</p><ul><li><strong>输入</strong>(你可以把它命名为<code>orders</code> ): Azure Stream Analytics将把它作为订单数据的(流)“源”</li><li><strong>输出</strong>(你可以将它命名为<code>customer-orders</code> ): Azure Stream Analytics将使用它作为一个“接收器”来存储查询处理的丰富数据</li></ul><h3 id="h-azure-blob-storage">Azure Blob存储</h3><p>你需要创建一个Azure存储帐户。<a href="https://docs.microsoft.com/azure/storage/common/storage-account-create?tabs=azure-portal&amp;WT.mc_id=acloudguru-blog-abhishgu" target="_blank" rel="noreferrer noopener">这个快速入门</a>带你完成这个过程，并为Azure门户、Azure CLI等提供指导。一旦完成，继续前进，<a href="https://docs.microsoft.com/azure/storage/blobs/storage-quickstart-blobs-portal?WT.mc_id=acloudguru-blog-abhishgu#create-a-container" target="_blank" rel="noreferrer noopener">使用Azure门户或<a href="https://docs.microsoft.com/azure/storage/blobs/storage-quickstart-blobs-cli?WT.mc_id=acloudguru-blog-abhishgu#create-a-container" target="_blank" rel="noreferrer noopener"> Azure CLI </a>创建一个容器</a>。</p><p>将下面的JSON保存到一个文件中，并上传到您刚刚创建的存储容器中。</p><pre class="wp-block-preformatted">[
    {
        "id": "1",
        "name": "Willis Collins",
        "location": "Dallas"
    },
    {
        "id": "2",
        "name": "Casey Brady",
        "location": "Chicago"
    },
    {
        "id": "3",
        "name": "Walker Wong",
        "location": "San Jose"
    },
    {
        "id": "4",
        "name": "Randall Weeks",
        "location": "San Diego"
    },
    {
        "id": "5",
        "name": "Gerardo Dorsey",
        "location": "New Jersey"
    }
]</pre><h3 id="h-azure-stream-analytics">Azure流分析</h3><p>首先创建一个Azure流分析作业。如果你想使用Azure门户，只需遵循本节概述的<a href="https://docs.microsoft.com/azure/stream-analytics/stream-analytics-quick-create-portal?WT.mc_id=acloudguru-blog-abhishgu#create-a-stream-analytics-job" target="_blank" rel="noreferrer noopener">步骤</a>或者如果你不喜欢点击UI，使用<a href="https://docs.microsoft.com/azure/stream-analytics/quick-create-azure-cli?WT.mc_id=acloudguru-blog-abhishgu#create-a-stream-analytics-job" target="_blank" rel="noreferrer noopener"> Azure CLI代替</a>。</p><p><strong>配置Azure事件中心输入</strong></p><p>打开您刚刚创建的Azure Stream Analytics作业，并将Azure Event Hubs配置为一个<strong>输入</strong>。以下是一些屏幕截图，应该可以指导您完成这些步骤:</p><p>从左侧菜单中选择<strong>输入</strong></p><p>选择<strong> +添加流输入&gt;事件中心</strong></p><p>输入事件中心详细信息-门户为您提供了从现有事件中心命名空间和订阅中的相应事件中心中进行选择的便利，因此您只需选择正确的名称即可。</p><p><strong>配置Azure Blob存储输入:</strong></p><p>从左侧菜单中选择<strong>输入</strong></p><p>选择<strong>添加参考输入&gt;斑点存储</strong></p><p>输入/选择Blob存储详细信息</p><p>一旦完成，您应该会看到以下<strong>输入</strong>:</p><h2 id="h-configure-the-query-and-test-with-sample-data">使用示例数据配置查询和测试</h2><p>Azure Stream Analytics允许你用样本数据测试你的流查询。在本节中，我们将分别为事件中心和Blob存储输入上传订单和客户信息的样本数据。</p><h3 id="h-upload-sample-data-for-orders"><strong>上传样本数据为</strong> <code><strong>orders:</strong></code> <strong/></h3><p>打开Azure Stream Analytics作业，选择<strong>查询</strong>并上传示例订单数据以供事件中心输入</p><p>将下面的JSON保存到一个文件并上传。</p><pre class="wp-block-preformatted">[
    {
        "id": "42",
        "custid": "1",
        "amount": "100"
    },
    {
        "id": "43",
        "custid": "2",
        "amount": "200"
    },
    {
        "id": "44",
        "custid": "3",
        "amount": "300"
    },
    {
        "id": "45",
        "custid": "3",
        "amount": "150"
    },
    {
        "id": "46",
        "custid": "4",
        "amount": "170"
    },
    {
        "id": "47",
        "custid": "5",
        "amount": "150"
    },
    {
        "id": "48",
        "custid": "5",
        "amount": "200"
    }

]</pre><h3 id="h-upload-sample-data-for-customers"><strong>上传客户样本数据</strong></h3><p>打开Azure Stream Analytics作业，选择<strong>查询</strong>并为Blob存储输入上传样本订单数据</p><p>您可以上传之前上传到Blob存储的JSON文件。</p><p>现在，配置并运行以下查询:</p><pre class="wp-block-preformatted">SELECT o.id as order_id, o.amount as purchase, o.custid as customer_id, c.name customer_name, c.location as customer_location
FROM orders o
JOIN customers c  
ON o.custid = c.id
</pre><p>打开Azure Stream Analytics作业，选择<strong>查询</strong>,然后按照下面截图中描述的步骤操作:</p><p>选择<strong>查询&gt;进入查询&gt;测试查询</strong>别忘了选择<strong>保存查询</strong></p><p>查询<code>JOINs</code>根据匹配的客户ID(在<code>customers</code>数据集中是<code>id</code>,在<code>orders</code>流中是<code>id</code>)用静态引用<code>customers</code>数据(来自Blob存储)对来自事件中心的数据进行排序。)</p><p><em>探究</em> <a href="https://docs.microsoft.com/stream-analytics-query/reference-data-join-azure-stream-analytics?WT.mc_id=acloudguru-blog-abhishgu"> <em>引用数据</em> <em>加入</em> <em>操作</em> </a> <em>或者挖掘到</em> <a href="https://docs.microsoft.com/stream-analytics-query/stream-analytics-query-language-reference?WT.mc_id=acloudguru-blog-abhishgu"> <em>流分析查询引用</em> </a></p><h2 id="h-test-the-job-with-streaming-data">使用流数据测试作业</h2><p>很高兴能够使用样本数据来测试我们的流解决方案。让我们继续尝试将实际数据(订单)端到端地流入活动中心。</p><p>运行<code>Job</code>需要一个<code>Output</code>。为了配置输出，选择<strong>输出&gt; +添加&gt;事件中枢</strong></p><p>输入事件中心详细信息:门户为您提供了从现有事件中心名称空间和订阅中的相应事件中心中进行选择的便利，因此您只需选择正确的名称空间即可。</p><h3 id="h-start-the-job">开始工作</h3><p>在Azure Stream Analytics界面，选择<strong>概览</strong>，点击<strong>开始</strong>并确认</p><p>等待作业开始，您应该看到<code>Status</code>变为<strong>运行</strong></p><h3 id="h-test-the-end-to-end-flow"><strong>测试端到端流量</strong> <strong/></h3><p>为了简单起见，我们可以使用<code>kafkacat</code> CLI来生成订单和消费丰富的事件(而不是程序)。只需<a href="https://github.com/edenhill/kafkacat#install" target="_blank" rel="noreferrer noopener">安装它</a>，你就可以开始了。</p><p><em>注意:虽然我使用了<code>kafkacat</code>，但是您可以自由选择任何其他机制(CLI或编程)。这个</em> <a href="https://docs.microsoft.com/azure/event-hubs/apache-kafka-developer-guide?WT.mc_id=%20acloudguru-blog-abhishgu"> <em>文档提供了大量的例子</em> </a></p><p>创建一个包含事件中心信息的<code>kafkacat.conf</code>文件:</p><pre class="wp-block-preformatted">metadata.broker.list=<namespace>&lt;namespace&gt;.servicebus.windows.net:9093
security.protocol=SASL_SSL
sasl.mechanisms=PLAIN
sasl.username=$ConnectionString
sasl.password=Endpoint=sb://<enter namespace="">&lt;namespace&gt;.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=<enter access="" key=""/></enter></namespace></pre><p><strong>启动消费者收听来自事件中心的输出主题</strong></p><p>让我们首先启动将连接到输出主题(<code>customer-orders</code>)的消费者流程，该输出主题将从Azure Stream Analytics获取丰富的订单信息</p><p>在终端中:</p><pre class="wp-block-preformatted">export KAFKACAT_CONFIG=kafkacat.conf
kafkacat -b <enter namespace="">&lt;namespace&gt;.servicebus.windows.net:9093 -t customer-orders

//output
% Reading configuration from file kafkacat.conf
% Auto-selecting Consumer mode (use -P or -C to override)</enter></pre><p>这将阻塞，等待来自<code>customer-orders</code>的记录。</p><p><strong>在另一个终端，开始向<code>orders</code>主题</strong>发送订单信息</p><pre class="wp-block-preformatted">kafkacat -P -b <enter namespace="">&lt;namespace&gt;.servicebus.windows.net:9093 -t orders</enter></pre><p>您可以通过<code>stdout</code>发送订单数据。只需一次粘贴一个，并观察另一个终端的输出:</p><pre class="wp-block-preformatted">{"id": "22","custid": "1","amount": "100"}
{"id": "23","custid": "2","amount": "200"}
{"id": "24","custid": "3","amount": "300"}
{"id": "25","custid": "4","amount": "400"}
{"id": "26","custid": "15","amount": "500"}</pre><p>您在消费者终端上看到的输出应该类似于以下内容:</p><pre class="wp-block-preformatted">...
% Reached end of topic customer-orders [0] at offset 0
{"order_id":"22","purchase":"100","customer_id":"11","customer_name":"Willis Collins","customer_location":"Dallas"}

% Reached end of topic customer-orders [0] at offset 1
{"order_id":"23","purchase":"200","customer_id":"2","customer_name":"Casey Brady","customer_location":"Chicago"
...</pre><p>请注意订单信息现在是如何被客户数据(本例中是姓名、位置)丰富的。您可以随意使用本主题中的信息。例如，您可以将这个丰富的物化视图持久化到<a href="https://docs.microsoft.com/azure/stream-analytics/stream-analytics-define-outputs?WT.mc_id=devto-blog-abhishgu#azure-cosmos-db"> Azure Cosmos DB </a>，触发一个<a href="https://docs.microsoft.com/azure/stream-analytics/stream-analytics-define-outputs?WT.mc_id=devto-blog-abhishgu#azure-functions"> Azure函数</a>等等。</p><p>不出所料，您不会看到与客户(其ID不在参考客户数据(在Blob存储中)中)下的订单相对应的丰富事件，因为<em> JOIN </em>标准是基于客户ID的。</p><p>这就把我们带到了本教程的结尾！我希望它能帮助你开始使用Azure Stream Analytics，并在进入更复杂的用例之前进行测试。</p><h2 id="h-where-to-go-next">接下来去哪里？</h2><p>除此之外，还有大量的材料供你挖掘。</p><h2 id="h-conclusion">结论</h2><p>高速实时数据带来了使用传统架构难以应对的挑战，其中一个问题就是<em>连接</em>这些数据流。根据不同的用例，定制的解决方案可能会更好地为您服务，但是这需要花费大量的时间和精力来做好。如果可能的话，您可能想要考虑提取您的数据处理架构的一部分，并将繁重的工作交给为这类问题定制的服务。</p><p>在这篇博文中，我们探索了一种可能的解决方案，通过结合使用Azure Event Hubs获取数据和Azure Stream Analytics使用SQL处理数据来实现流连接。这些都是功能强大的现成服务，您可以配置和使用它们，而无需设置任何基础架构，而且由于有了云，这些解决方案中涉及的分布式系统的底层复杂性完全从我们这里抽象出来。</p><p class="has-text-align-center"><em>想了解更多关于Azure认证的信息吗？</em> <br/> <em>查看我们的<a href="https://acloudguru.com/azure-cloud-training" target="_blank" rel="noreferrer noopener"> Azure认证和学习路径。</a> </em></p></div></div>    
</body>
</html>
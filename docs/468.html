<html>
<head>
<title>Building a serverless Kafka data pipeline | A Cloud Guru</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>构建无服务器的Kafka数据管道</h1>
<blockquote>原文：<a href="https://acloudguru.com/blog/engineering/building-a-serverless-kafka-data-pipeline#0001-01-01">https://acloudguru.com/blog/engineering/building-a-serverless-kafka-data-pipeline#0001-01-01</a></blockquote><div><div class="elementor-widget-container"><p>无服务器服务允许我们构建应用程序，而不必担心底层基础设施。就Apache Kafka而言，这允许开发人员避免配置、扩展和管理其集群的资源利用。在这篇文章中，我将使用亚马逊MSK无服务器集群创建一个无服务器Kafka集群，并展示如何创建无服务器生产者和消费者。</p><hr class="wp-block-separator"/><h2 class="has-text-align-center" id="h-accelerate-your-career">加速您的职业发展</h2><p class="has-text-align-center"><a href="https://acloudguru.com/pricing">从ACG开始</a>通过AWS、Microsoft Azure、Google Cloud等领域的课程和实际动手实验室改变你的职业生涯。</p><hr class="wp-block-separator"/><h2 id="h-how-to-start-a-serverless-kafka-cluster">如何启动<strong>无服务器Kafka集群</strong></h2><p>Apache Kafka是一个分布式事件存储和流处理平台。因为它具有高度的可伸缩性和弹性，所以它提供了一种将数据处理与源和目标分离的机制。</p><p>Kafka可以根据您的工作负载进行定制，这会带来运营开销。使用无服务器Kafka，代价是您失去了配置集群容量的灵活性，但您获得了通过单一界面使用Kafka的能力，该界面只为客户端提供一个端点。</p><p>要开始使用AWS，您可以从亚马逊MSK控制台页面创建一个集群。</p><figure class="wp-block-image"><img loading="lazy" src="../Images/a36e4454aee0b39159d21ba0283a0285.png" alt="image.png" data-original-src="https://lh6.googleusercontent.com/4k1n--_aV3PAEMa2x9RuEN7hCxdBTBT1QA_dRwUdiTHT_lXDSqvWa22JPjS0WeLKnErRQ362K4m3RswVB6Uq7-fsTAhJ8awCNWP-DJfPo2FODP0ifhtgpkDvjMEl7ykBj2l0VMZ51IRABl28og"/></figure><p>有了无服务器，就不用担心配置了。您只需继续操作，几分钟内就可以设置好集群。</p><figure class="wp-block-image"><img loading="lazy" src="../Images/361f18dbe21c8289a25b195c15790ff5.png" alt="image.png" data-original-src="https://lh5.googleusercontent.com/ENohGQXlGWtHXUcPy8PRjSkR7xA52Th7MCdDYy2PYwVEhyNlHmeOKJOkA0yuj2-qVziW98IKR8-tEQlv5Uu4PtAeTTuFn41eLIAuFLU2dHPKNCMfCi9nhvaRirtbLOQtxA1Os8Wd9ntkKhqycA"/></figure><p>当您不知道需要处理多少数据时，入口的默认选项200 Mib/s和400 Mib/s非常适合Kafka工作负载的初学者。如果您的工作负载增加，并且无服务器集群无法满足默认的写/读吞吐量，您可以探索使用<a href="https://aws.amazon.com/msk/" target="_blank" rel="noreferrer noopener">托管的AWS Kafka集群</a>，它提供了针对您的工作负载调整集群的灵活性。</p><p>集群启动后，您可以在控制台的properties选项卡中查看客户机所需的API端点。</p><figure class="wp-block-image"><img loading="lazy" src="../Images/68e6c7969cb7981e79f4711591724021.png" alt="image.png" data-original-src="https://lh6.googleusercontent.com/sffOFDWH495Psiu-NDlxzRdvA2EVgksMhKDOWtZhC84idx_h3vEnzTYbEIVOZ5Lr6tPDzAAPLOoCJcOV57FyQLEvMiLOh-v5IplP23DUS308H5h2MdOcFh35_zGR2EOgkHIcY-8ldf6JbVxskA"/></figure><p>有关设置集群的更多信息，请查看官方<a href="https://docs.aws.amazon.com/msk/latest/developerguide/serverless-getting-started.html" target="_blank" rel="noreferrer noopener">文档</a>。</p><h2 id="h-creating-serverless-kafka-clients"><strong>创建无服务器Kafka客户端</strong></h2><p>一旦集群启动，我们就可以创建Kafka客户端，将数据发送到集群，并从集群中进行消费。在本节中，我将解释如何:</p><ul><li>为Lambda函数设置所需的IAM权限</li><li>为Lambda函数构建一个Docker映像</li><li>配置Lambda函数以与无服务器Kafka集群通信</li></ul><h3 id="h-setting-permissions"><strong>设置权限</strong></h3><p>目前，与集群通信需要基于IAM的身份验证。以下是可用于客户端的示例策略。确保用您的值替换<code>REGION</code>、<code>Account-ID</code>和<code>CLUSTER_NAME</code>。</p><pre class="wp-block-code"><code>{
   "Version": "2012-10-17",
   "Statement": [
       {
           "Effect": "Allow",
           "Action": [
               "kafka-cluster:Connect",
               "kafka-cluster:AlterCluster",
               "kafka-cluster:DescribeCluster"
           ],
           "Resource": [
               "arn:aws:kafka:REGION:Account-ID:cluster/CLUSTER_NAME/*"
           ]
       },
       {
           "Effect": "Allow",
           "Action": [
               "kafka-cluster:*Topic*",
               "kafka-cluster:WriteData",
               "kafka-cluster:ReadData"
           ],
 "Resource": [
               "arn:aws:kafka:REGION:Account-ID:topic/CLUSTER_NAME/*"
           ]
       },
       {
           "Effect": "Allow",
           "Action": [
               "kafka-cluster:AlterGroup",
               "kafka-cluster:DescribeGroup"
           ],
           "Resource": [
               "arn:aws:kafka:REGION:Account-ID:group/CLUSTER_NAME/*"
           ]
       }
   ]
}</code></pre><p>接下来，我们可以为Lambda函数创建一个新角色，并将上面的策略附加到该角色。</p><p><img loading="lazy" src="../Images/78ad00336a8a502da47eba0ecf2a8882.png" alt="image.png" data-original-src="https://lh6.googleusercontent.com/tBB1qts9NXLTKOjxO4133hDCthhnhl-7dAaDjAca86SwA4l3bvzeOSu9d81_dHR97OR5Zl6l86eyWipSxy67sRDhxlXHPWzzXm0dcuwVIRdH_e3bNub2fgNuAyzTHxUBqfIc8pGjnQqOgqcTFw"/></p><p>此外，该角色将需要AWSLambdaVPCAccessExecutionRole策略，因为该功能需要部署在与无服务器集群相同的VPC中。</p><figure class="wp-block-image"><img loading="lazy" src="../Images/ebaee5315b250ed5130af52385796ada.png" alt="image.png" data-original-src="https://lh4.googleusercontent.com/BUka8vCAqOS0TWpK1D5LwGj4YwxQ3VCFL3_VWQ57TPvlVnzJmAr_Zv17trlHunxwnAJEgPojCTvRvpAJuNweyQmbO1rfPTgOlNrKy6VuIvbHcT6Z2_Ad-ffMW5sZj4_vjyGUiwQmNC4cTInHUA"/></figure><h4><strong>构建Docker图像</strong></h4><p>对于我们的Lambda函数，我们将使用一个定制容器，它具有连接到Kafka集群所需的所有先决条件。下面是一个可以用来构建Kafka客户端Lambda函数的Dockerfile示例。查看底部的参考资料部分，获取<code>handler.py</code>和<code>client.properties</code>文件。</p><pre class="wp-block-code"><code># Lambda base image
FROM public.ecr.aws/lambda/python:3.8
# Install Kafka prereqs
RUN yum -y install java-11 wget tar
RUN wget https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz
RUN tar -xzf kafka_2.12-2.8.1.tgz
RUN wget https://github.com/aws/aws-msk-iam-auth/releases/download/v1.1.1/aws-msk-iam-auth-1.1.1-all.jar
RUN mv aws-msk-iam-auth-1.1.1-all.jar ${LAMBDA_TASK_ROOT}/kafka_2.12-2.8.1/libs
COPY ./client.properties ${LAMBDA_TASK_ROOT}
# Remove tgz
RUN rm kafka_2.12-2.8.1.tgz
# Lambda code
COPY handler.py ${LAMBDA_TASK_ROOT}
# Run handler
CMD ["handler.action"]</code></pre><p>按照这里的指示<a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html" target="_blank" rel="noreferrer noopener"/>将你的图片推送到<a href="https://aws.amazon.com/ecr/" target="_blank" rel="noreferrer noopener">亚马逊弹性容器注册处</a> (ECR)。一旦你的图像在ECR中，你可以使用容器图像创建一个<a href="https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-images.html" target="_blank" rel="noreferrer noopener"> Lambda函数。创建函数后，我们需要更新一些设置。</a></p><h3 id="h-configuring-the-lambda-function"><strong>配置Lambda功能</strong></h3><p>最后，我们需要配置Lambda函数，以便它可以与集群交互。对于基本设置，我们需要将内存增加到1024 MB，超时时间增加到1分钟，并使用我们上面创建的IAM角色。</p><p><img loading="lazy" src="../Images/e50ecc0b2cdf57ac21c7e8efd32dcf70.png" alt="image.png" data-original-src="https://lh5.googleusercontent.com/WCPlUqF6xRgJ5TPwspd64Cr2R-1HPNX3ynF7Uoz29hgbIR38bpAWrQr2-wVaPsF4Wvi_HioEe-Ce6F-sNXiR1ukNCz8zThEVRHvB0NHpSb1iKwlNG5pX5BcUeJa_qDOn-O4zcgihzjJuzv8stA"/></p><p>接下来，我们需要确保Lambda函数与MSK集群具有相同的VPC、子网和安全组。</p><p><img loading="lazy" src="../Images/5fc953e740cecde630162620f2439f7a.png" alt="image.png" data-original-src="https://lh6.googleusercontent.com/vr3EjQkAjQsDrAGZYHuP02kH4dhLW1W_Y6ktzEZkZhs5kYEjVmrUfbg1tQ_ZO9vLcoSS0YSPrugssIJlQ4NSuFtFXuEXUeG-PF8z3bKk9kAhbDYrmqqXQyJs2lsjhpXvi_I71YP1Xh7i9j_u_g"/></p><p>最后，我们需要从“查看客户端信息”按钮添加设置为MSK集群端点的<code>KAFKA_ENDPOINT</code>环境变量。</p><figure class="wp-block-image"><img loading="lazy" src="../Images/b3c1b64718ccd8c6e3988019f7103a7d.png" alt="image.png" data-original-src="https://lh5.googleusercontent.com/pkA5eYKMPiPwlnvWJFiE5fIxEaqW5csr_f2T-l-1vHWo9qCYewM-OTHJId3tG29dQmbIr8F7vc9J_OwZFxY5LrbFn2u0Um4bOCLmkxYixZWoPTh6NWGUgHIhiFnKlHFZhJ5YAppMuFwPruHLnA"/></figure><p>这样，我们就可以开始生产和消费数据了。</p><h2 id="h-producing-and-consuming-data"><strong>生产和消费数据</strong></h2><p>使用Lambda，我们可以通过控制台中的“test”选项卡发送测试事件，以验证我们的容器映像正在工作。为了测试produce函数，我们可以将这个简单的有效载荷发送到<code>my_topic</code>。</p><pre class="wp-block-code"><code>{
 "action": "produce",
 "topic": "my_topic",
}</code></pre><p>生产者代码只是使用样本数据，通过传入的参数调用生产者脚本。</p><pre class="wp-block-code"><code>./kafka_2.12-2.8.1/bin/kafka-console-producer.sh 
--topic {topic}
--bootstrap-server {KAFKA_ENDPOINT}
--producer.config client.properties &lt; /tmp/test.json</code></pre><p>一旦代码运行了来自MSK集群的指标，将需要几分钟来更新和指示数据已收到。可以从集群主页上的指标选项卡中查看指标。</p><p><img loading="lazy" src="../Images/e741d148a7140c17cdfc1de4efb0f013.png" alt="image.png" data-original-src="https://lh3.googleusercontent.com/1iam5UZ3x2lrcy2qvO2t2ojZ9RtG2hdZ7telnPRRvc00eLR0UO-vY-qC8lIksR0h24zrbYXY_BrNKinVa1PAn0uPVOpLOfgPxL2tkvMtjKzP1hRiTaE60a-i5V9s18c0f58WIYKE12pC_qImug"/></p><p>然后，我们还可以通过更新有效负载来测试消费者代码。</p><pre class="wp-block-code"><code>{
 "action": "consume",
 "topic": "my_topic",
}</code></pre><p>类似地，消费者代码使用传入的参数调用消费者脚本，并将输出写入示例文件。根据您的使用情况，可以将文件上传到S3，或者对数据运行其他功能。超时参数用于确保脚本关闭，否则它会一直等待输入。</p><pre class="wp-block-code"><code>"./kafka_2.12-2.8.1/bin/kafka-console-consumer.sh
--topic {topic} --from-beginning
--bootstrap-server {KAFKA_ENDPOINT}
--consumer.config client.properties --timeout-ms 12000 &gt; /tmp/output.json"</code></pre><p>所有的代码都是可配置的，以适应您的用例，所以请随意使用它作为入门指南，并根据需要进行调整。</p><h2 id="h-conclusion">结论</h2><p>在本帖中，我们讨论了如何通过以下方式成功设置开始扩展无服务器Kafka管道所需的基础设施:</p><ul><li>启动无服务器MSK集群</li><li>创建Kafka客户端Docker映像</li><li>部署基于容器的Lambda函数</li><li>通过Lambda控制台生成和消费数据</li></ul><p>这个解决方案可以扩展以适应不同的用例，比如使用Eventbridge定期从数据库中读取数据，或者将处理过的消息上传到S3。Lambda的灵活性与Kafka将数据处理从源和目标解耦的能力相结合，为无服务器数据管道提供了基础。</p><p><em>在Twitter上关注Banjo，点击</em> <a href="https://twitter.com/banjtheman" target="_blank" rel="noreferrer noopener"> <em>，@banjtheman </em> </a> <em>和</em> <a href="https://twitter.com/awsdevelopers" target="_blank" rel="noreferrer noopener"> <em>，@AWSDevelopers </em> </a> <em>了解更多关于云计算和AWS的有用提示和技巧。</em></p><h3 id="h-about-the-author">关于作者</h3><p>Banjo是AWS的一名高级开发人员，他在那里帮助开发人员对使用AWS感到兴奋。Banjo热衷于将数据操作化，并围绕利用数据启动了一个播客、一个meetup和开源项目。当没有建造下一个大东西时，班卓喜欢通过玩视频游戏特别是JRPGs和探索他周围发生的事件来放松</p><h2 id="h-resources">资源</h2><p><strong>客户端属性</strong></p><pre class="wp-block-code"><code>security.protocol=SASL_SSL
sasl.mechanism=AWS_MSK_IAM
sasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required;
sasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler</code></pre><p><strong>handle . py</strong></p><p>这段代码为Lambda提供了一个模板，用于与无服务器Kafka集群进行交互。由于基于IAM的认证，标准的Kafka Python库不能与集群交互，所以我开发了一个Java脚本包装器来与集群交互。</p><pre class="wp-block-code"><code>import json
import logging
import os
import subprocess
from typing import List, Dict, Any

KAFKA_ENDPOINT = os.environ["KAFKA_ENDPOINT"]


def action(event, context) -&gt; Dict[str, Any]:
   """
   Purpose:
       Entrypoint for action on cluster
   Args:
       event - data from lambda
       context - context data from lambda
   Returns:
       response - JSON response
   """

   body = {
       "message": "Running Action",
       "input": event,
   }

   curr_action = event["action"]
   topic = event["topic"]

   # If creating topic, can specify number of partitions
   if "num_partitions" in event:
       num_partitions = event["num_partitions"]
   else:
       num_partitions = 1

   if curr_action == "produce":
       response = produce(topic, num_partitions)
   elif curr_action == "consume":
       response = consume(topic)
   else:
       raise ValueError("Invalid action")

   return response


def consume(topic: str) -&gt; Dict[str, Any]:
   """
   Purpose:
       Consume data in a topic
   Args:
       topic - topic to consume on
   Returns:
       response - JSON response
   """
   body = {
       "message": "Data consumed!!!",
   }

   # TODO can play with the timeout, on how long you want to collect data
   # TODO can also configure if you want to get data from the beginning or not
   cmd = f"./kafka_2.12-2.8.1/bin/kafka-console-consumer.sh --topic {topic} --from-beginning --bootstrap-server {KAFKA_ENDPOINT} --consumer.config client.properties --timeout-ms 12000 &gt; /tmp/output.json"

   os.system(cmd)

   # TODO
   # Do what you need to do with output.json i.e upload to s3, run analytics, etc..

   response = {"statusCode": 200, "body": json.dumps(body)}
   logging.info(response)

   return response


def produce(topic: str, num_partitions: int) -&gt; Dict[str, Any]:
   """
   Purpose:
       Produce data in a topic
   Args:
       topic - topic to create
       num_partitions - number of num_partitions to use
   Returns:
       response - JSON response
   """
   # TODO would input your process to get data to send to topic
   sample_data = '{"user":"Alice","number":105,"timestampInEpoch":1650880895}\n{"user":"Bob","number":5,"timestampInEpoch":1650880324}'

   # Write sample output to temp file
   write_to_file("/tmp/test.json", sample_data)

   body = {
       "message": "Data produced!!!",
       "input": sample_data,
   }

   # Check if topic exists, if not create it
   topics = list_topics()
   if not topic in topics:
       if not create_topic(topic, num_partitions):
           raise RuntimeError("Topic not created")

   produce_topic_command = f"./kafka_2.12-2.8.1/bin/kafka-console-producer.sh  --topic {topic} --bootstrap-server {KAFKA_ENDPOINT} --producer.config client.properties &lt; /tmp/test.json"

   os.system(produce_topic_command)

   response = {"statusCode": 200, "body": json.dumps(body)}
   logging.info(response)

   return response


def create_topic(topic:str, num_partitions:int) -&gt; bool:
   """
   Purpose:
       Create topic in cluster
   Args:
       topic - topic to create
       num_partitions - number of num_partitions to use
   Returns:
       bool - True if created, false if not
   """
  
   cmd = f"./kafka_2.12-2.8.1/bin/kafka-topics.sh --bootstrap-server {KAFKA_ENDPOINT} --command-config client.properties --create --topic {topic} --partitions {num_partitions}"

   output = subprocess.check_output(cmd, shell=True)
   output_string = output.decode("utf-8")
   outputs = output_string.split("\n")

   # Check if created
   success_string = f"Created topic {topic}."
   if success_string in outputs:
       logging.info(outputs)
       return True
   else:
       logging.error(outputs)
       return False


def list_topics() -&gt; List[str]:
   """
   Purpose:
       List topics in cluster
   Args:
       N/A
   Returns:
       topics - list of topics
   """
   cmd = f"./kafka_2.12-2.8.1/bin/kafka-topics.sh --list --bootstrap-server {KAFKA_ENDPOINT} --command-config client.properties"

   # Run the command to get list of topics
   output = subprocess.check_output(cmd, shell=True)
   output_string = output.decode("utf-8")
   topics = output_string.split("\n")  # turn output to array

   return topics


def test_produce_consume() -&gt; None:
   """
   Purpose:
       Test producing and consuming
   Args:
       N/A
   Returns:
       N/A
   """
   logging.info("testing produce")
   response = produce("my_topic", 1)
   logging.info(response)
   logging.info("testing consume")
   consume("my_topic")
   logging.info("Done and Done")


def write_to_file(file_path: str, file_text: str) -&gt; bool:
   """
   Purpose:
       Write text from a file
   Args/Requests:
        file_path: file path
        file_text: Text of file
   Return:
       Status: True if written, False if failed
   """

   try:
       with open(file_path, "w") as myfile:
           myfile.write(file_text)
           return True

   except Exception as error:
       logging.error(error)
       return False


if __name__ == "__main__":
   loglevel = logging.INFO
   logging.basicConfig(format="%(levelname)s: %(message)s", level=loglevel)
   test_produce_consume()</code></pre><p/></div></div>    
</body>
</html>
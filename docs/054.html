<html>
<head>
<title>How to build a serverless web crawler | A Cloud Guru</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>如何建立一个无服务器的网络爬虫</h1>
<blockquote>原文：<a href="https://acloudguru.com/blog/engineering/how-to-build-a-serverless-web-crawler#0001-01-01">https://acloudguru.com/blog/engineering/how-to-build-a-serverless-web-crawler#0001-01-01</a></blockquote><div><div class="elementor-widget-container"><h2 id="h-use-serverless-to-scale-an-old-concept-for-the-modern-era">使用无服务器扩展现代的旧概念</h2><p id="b430">最近，一个客户项目需要抓取一个大型媒体网站，以生成URL和网站资产的列表。鉴于网站的规模很大，使用传统的爬行方法会使Lambda函数超时——所以我们考虑将方法改为无服务器模型。</p><p id="8988">以下是我在这个项目中学到的关于设计无服务器功能的知识。</p><h3 id="6fb8">我们跑之前先爬吧</h3><p id="e39f">抓取一个网站有很多原因——抓取不同于抓取。在爬行一个站点时，我们登陆一个网页，通常是主页，搜索网页上的URL，然后递归地浏览这些URL。</p><p id="4bc4">抓取可能是爬行的原因——特别是如果您想要存储这些页面内容的副本，或者您可能只是出于索引页面的一些次要原因。</p><div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" src="../Images/51232ea8484c25bc6246699bf593bb51.png" alt="" class="wp-image-42057" data-original-src="https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2020/12/1_AG-bH8WI5olzaXen-troOw.png"/><figcaption>Traversing the tree of a typical website</figcaption></figure></div><p id="3cbd">爬行是有趣的部分，因为您可以快速生成大量的URL列表，或者通过实施一些规则来控制您收集的内容。例如，您可能只浏览具有相同域名的URL，并删除查询参数以减少重复。</p><p id="7ace">然后你必须考虑抓取的速度——你是一次处理一个URL还是同时浏览多个？如果你在html中发现混合用法，你如何看待http和https？</p><p id="5b04">对于Node用户来说，有一个优雅地称为<a href="https://www.npmjs.com/package/website-scraper" target="_blank" rel="noreferrer noopener">网站抓取器</a>的包，它有大量的配置选项来处理这些问题，还有许多其他功能。</p><p id="1436">运行这个工具非常简单——你可以访问<a href="https://github.com/jbesw/askJames-serverlessCrawler/blob/master/example.js" target="_blank" rel="noreferrer noopener"> my github repo </a>获取完整的例子，但是这里有一些重要的部分:</p><pre class="wp-block-code"><code>const options = {
  urls: myTargetSite,
  directory: '/temp/',
  prettifyUrls: true,
  recursive: true,
  filenameGenerator: 'bySiteStructure',
  urlFilter: (url) =&gt; url.startsWith(myTargetSite),
  onResourceSaved: (resource) =&gt; URLs.push(resource.url),
  onResourceError: (resource, err) =&gt; console.error(`${resource}: ${err}`),
  requestConcurrency: 10
}
const result = await crawl(options)
console.log('# of URLs:', URLs.length)</code></pre><p id="558b">这个包主要是配置驱动的。我们指定一个目标网站，让它递归地搜索与<em> urlFilter匹配的URL。</em>在本例中，过滤器被设置为包括同一域中的任何URL，同时请求多达10个URL。</p><p id="e717">当保存一个资源时，我们将URL推送到一个数组上，然后记录任何错误。在过程结束时，所有发现的资源都存储在数组<em>URL</em>中。完整的脚本中有更多的代码，但这些是最基本的。</p><p id="cbea">这一切都很棒——但是当抓取一个超过10，000个URL的网站时，我们遇到了一些主要问题。它有效地作为一个原子作业运行，获取URL，节省资源，并管理要浏览的URL的内部列表:</p><ul><li>它可以运行几个小时，如果它失败了，没有办法从它停止的地方恢复。所有的状态都是内部管理的，如果出现问题就会丢失。</li><li>随着内部映射的增长，消耗的内存可能会很大，因此您必须确保所使用的实例分配有足够的RAM。</li><li>对于大型站点，使用的临时磁盘空间可能高达数百GB，因此您需要确保有足够的本地可用空间。</li></ul><p id="b199">所有这些都不是webscraper包的错——纯粹是因为<em>一切都在大规模变化</em>。这个项目对于互联网上99%的网站来说都是完美的，但是对于抓取纽约时报或者维基百科来说就不是了。</p><p id="f190">鉴于客户网站的规模，我们需要重新考虑这种方法，同时利用无服务器的优势。</p><h3 id="ebe7">无服务器Crawler —版本1.0</h3><p id="b21b">第一步是简单地将webrawler代码打包成一个Lambda函数，放在一个基本的“提升移位”中。这一尝试按预期运行——当站点探测不能及时完成时，在15分钟时超时。</p><p id="8faf">在15分钟结束时，除了你可以从我们的日志文件的灰烬中收集到的信息之外，我们对进度没有什么概念。为了确保它的工作，我们必须确保网站的大小与功能运行的时间长短没有根本的关系。</p><p id="45b0">在新版本中，该函数检索页面，找到与我们的过滤器匹配的URL，将它们存储在某个地方，然后终止。当新的URL被存储时，这将启动相同的函数，并且该过程重新开始。</p><figure class="wp-block-image size-large"><img loading="lazy" src="../Images/a7c9b3ff9f3dba24ac4784fb09ce6dd3.png" alt="" class="wp-image-42055" data-original-src="https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2020/12/1_4Bjyz1I7OzyTMVR7JGqu2g.png"/></figure><p>简而言之，我们将从代码中提取递归，并用我们的无服务器玩具盒复制它。如果我们将URL存储在DynamoDB中，我们可以在每次将URL写入数据库时使用流来触发爬虫:</p><p id="145c">我们构建了这个，所以整个过程是通过将主页URL作为记录写入DynamoDB来触发的。事情是这样的:</p><ol><li>将“https://mytestsite.com”写入DynamoDB。</li><li>该流导致Lambda以传入事件“https://mytestsite.com”开始。</li><li>它获取页面，找到20个URL并在DynamoDB中保存为20条记录。</li><li>20个新记录的流导致多个Lambdas开始。每个加载一个页面，找到另外20个URL并将它们写入数据库。</li><li>400个新记录的流导致多个lambda启动，使我们正在爬行的网站崩溃，并且所有未完成的lambda都抛出错误。</li></ol><p id="e7c6">好吧，那是哪里出了问题？</p><p id="44de">人们很容易忘记<em>Lambda如何高效地为您扩展— </em>，这可能会导致无意和自我诱导的拒绝服务(DOS)。</p><p id="79d0">无论网站有多大，总有一个并发水平会使网络服务器不堪重负。我们需要对这种工作方式进行调整，以防止Lambdas尽职尽责地完成工作。我们需要使网络爬虫，嗯…爬行得更快。</p><h3 id="eaeb">无服务器网络爬虫2.0-现在更慢！</h3><p id="3b05">我们之前的问题是由Lambda完成其工作引起的。如果许多记录同时写入DynamoDB，这将激发更多的并发Lambdas来完成工作。</p><p id="fb26">有时候你不想要这种行为——这就是其中之一。那么如何才能让它慢下来呢？一般来说，有几个策略可以帮助刹车:</p><ul><li>在Lambda函数写入DynamoDB之间引入一个<a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html" target="_blank" rel="noreferrer noopener"> SQS延迟队列</a>——这增加了长达15分钟的可配置延迟。</li><li>将Lambda函数的并发设置从未保留的默认值(1000)更改为一个更小的值。将该值设置为1将阻止所有并发，并有效地导致串行接收。在我们的例子中，我们发现10是足够的。</li><li>更改批处理大小，这会影响每次调用从流中发送的记录数量。这里较低的数字会导致更多的并发调用，而如果同时添加许多记录，较大的数字会导致单个Lambda处理来自流的更多项。</li></ul><p id="e89a">在这三个层次之间，如果DynamoDB流中出现大量事件，我们现在有了合理的控制量来平滑处理速度。</p><h3 id="d2a7">给我看看代码！</h3><p id="577b">首先，在你运行任何东西之前，确保你有一个自己拥有的或者有权限抓取的小网站——最好是一个有少量页面的开发网站。在没有明确许可的情况下抓取第三方网站通常会违反使用条款，你也不想违反AWS的可接受使用政策。</p><p id="8fc7">好了，我们来看看包里的<a href="https://github.com/jbesw/askJames-serverlessCrawler" target="_blank" rel="noreferrer noopener">文件:</a></p><ul><li><strong>handler . js</strong>:Lambda的默认入口点，它将并发执行所有传入的事件——如果批处理大小是5个URL，它将同时获取所有这些。这里的<em>等待承诺。all </em>机制处理实现这一点的复杂性。</li><li><strong> processURL.js </strong>:这将获取页面和发现的URL，并将它们以25个项目为一组分批放入DynamoDB。</li><li><strong> crawl.js </strong>:实际的爬行工作在这里进行——它获取页面，然后使用Cheerio包发现html中的URL。有一些逻辑来验证URL并消除重复。</li><li><strong> dynamodb.js </strong>:它使用batchWriteItem一次上传25个项目到dynamodb表。</li><li><strong> test.js </strong>:模拟testEvent.json中新项目事件的最小测试工具，如果运行node test.js，它将启动testEvent中URL的整个过程:</li></ul><p id="d650">要使用这段代码，您需要创建一个名为<code><strong>crawler</strong></code>的DynamoDB表。此时，当代码运行时，它将只对单个URL有效。</p><p id="9747">要使它在新的URL添加到DynamoDB时触发，您必须激活表上的流——转到“概述”选项卡，启用流并将流ARN复制到<code><strong>serverless.yaml</strong></code>文件中。</p><p id="5e5d">是的，这些都可以在回购中自动完成。由于Lambda和表格交互的递归性质，我不希望任何人下载代码，对Amazon.com运行它，并想知道为什么他们的AWS账单是天价。一旦你将DynamoDB流连接到Lambda函数，你将有一个递归的无服务器循环<em>，所以要非常小心</em>。</p><p id="e87b">有一个有趣的地方不是很明显，但是有助于爬行过程。在我们的DynamoDB表中，URL是主分区键，因此它必须是惟一的。当batchWriteItem试图将重复值写入表中时，由于重复项无法更新项，因此不会写入重复项，因此不会触发流事件。这很关键，因为在每次迭代中发现的许多URL已经出现在表中。</p><p id="62ac">最终的整体流程如下所示:</p><h3 id="4080">无服务器网络爬虫3.0</h3><p id="853c">提供的代码只是外壳。对于我们客户的项目，我们还实现了更多的webscraper逻辑，每次调用Lambda时都为S3节省资源。我们还向DynamoDB表添加了其他属性，包括状态(例如“新”、“处理中”、“错误”)、时间戳以及使用cron作业跟踪失败页面的能力。</p><p id="d654">在无服务器环境中工作有许多有益的副作用:</p><ul><li>在S3上存储对象，与在EC2实例上存储文件相比，没有大小限制。它适用于任何规模的网站。</li><li>每次调用所需的RAM数量不受爬网总大小的影响。每个Lambda函数只关心下载单个页面。</li><li>如果目标站点变得不可用，或者整个过程由于某种原因而暂停，您会知道爬行的状态，因为它存储在DynamoDB表中。我们从流程中提取递归状态，并将其放入数据库。</li><li>扩展功能是微不足道的。例如，将图像与HTML或CSS文件区别对待不会影响原始功能。</li></ul><p id="5957">对我来说，这个项目最有趣的一点是意识到有多少不适合无服务器的长寿命流程可以利用相同的模式。</p><p id="b45e">例如，有可能在这些应用程序中识别出一些迭代元素，这些元素可以被分解，这样就可以在DynamoDB中维护状态。无服务器的并发能力可以大大提高这些任务的性能，而潜在的成本要低得多。</p></div></div>    
</body>
</html>